{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ensemble Methods & Boosting — Student Lab\n",
        "\n",
        "Week 4 introduces sklearn models, but you must still explain *why* they work (bias/variance)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification, load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "def check(name: str, cond: bool):\n",
        "    if not cond:\n",
        "        raise AssertionError(f'Failed: {name}')\n",
        "    print(f'OK: {name}')\n",
        "\n",
        "rng = np.random.default_rng(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 0 — Dataset (synthetic default, real optional)\n",
        "\n",
        "### Task 0.1: Choose dataset\n",
        "Use synthetic by default. Optionally switch to breast cancer dataset.\n",
        "\n",
        "# TODO: set `use_real = False` or True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: shapes\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1400, 20)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "use_real = False  # TODO\n",
        "\n",
        "if use_real:\n",
        "    data = load_breast_cancer()\n",
        "    X = data.data\n",
        "    y = data.target\n",
        "else:\n",
        "    X, y = make_classification(\n",
        "        n_samples=2000,\n",
        "        n_features=20,\n",
        "        n_informative=8,\n",
        "        n_redundant=4,\n",
        "        class_sep=1.0,\n",
        "        flip_y=0.03,\n",
        "        random_state=0,\n",
        "    )\n",
        "\n",
        "Xtr, Xva, ytr, yva = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
        "check('shapes', Xtr.shape[0]==ytr.shape[0] and Xva.shape[0]==yva.shape[0])\n",
        "Xtr.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1 — Baseline vs Trees vs Random Forest\n",
        "\n",
        "### Task 1.1: Train baseline decision tree vs random forest\n",
        "\n",
        "# TODO: Train:\n",
        "- DecisionTreeClassifier(max_depth=?)\n",
        "- RandomForestClassifier(n_estimators=?, max_depth=?, oob_score=True, bootstrap=True)\n",
        "\n",
        "Compute accuracy + ROC-AUC on validation.\n",
        "\n",
        "**Checkpoint:** Why does bagging reduce variance?\n",
        "\n",
        "Because averaging will cancel out errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tree (0.805, 0.8664611111111112)\n",
            "rf   (0.8916666666666667, 0.9509777777777777)\n",
            "rf oob_score 0.9135714285714286\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "tree = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
        "rf = RandomForestClassifier(n_estimators=300, max_depth=None, min_samples_leaf=2, oob_score=True)\n",
        "\n",
        "tree.fit(Xtr, ytr)\n",
        "rf.fit(Xtr, ytr)\n",
        "\n",
        "def eval_model(clf, X, y):\n",
        "    pred = clf.predict(X)\n",
        "    acc = accuracy_score(y, pred)\n",
        "    # many sklearn classifiers have predict_proba; handle if not\n",
        "    if hasattr(clf, 'predict_proba'):\n",
        "        proba = clf.predict_proba(X)[:, 1]\n",
        "        auc = roc_auc_score(y, proba)\n",
        "    else:\n",
        "        auc = float('nan')\n",
        "    return acc, auc\n",
        "\n",
        "print('tree', eval_model(tree, Xva, yva))\n",
        "print('rf  ', eval_model(rf, Xva, yva))\n",
        "\n",
        "if hasattr(rf, 'oob_score_'):\n",
        "    print('rf oob_score', rf.oob_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1.2: Feature importance gotcha\n",
        "\n",
        "Inspect `feature_importances_` and explain why correlated features can distort importances.\n",
        "\n",
        "# TODO: print top 10 features by importance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "top idx [12 15 17  3  7 11  4 18  1 13]\n",
            "top importances [0.12932485 0.12600767 0.12339085 0.12060583 0.10097197 0.09630965\n",
            " 0.05309302 0.03569581 0.03334735 0.03130096]\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "imp = rf.feature_importances_\n",
        "top = np.argsort(-imp)[:10]\n",
        "print('top idx', top)\n",
        "print('top importances', imp[top])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2 — Gradient Boosting\n",
        "\n",
        "### Task 2.1: Train GradientBoostingClassifier\n",
        "\n",
        "# TODO: Train GB with different n_estimators and learning_rate and compare.\n",
        "\n",
        "**Checkpoint:** Why can boosting overfit with too many estimators?\n",
        "\n",
        "when we make too many trees, they will try to capture the noise also. hence overfitting occurs when we use too many estimators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gb {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 2} (0.8616666666666667, 0.9393722222222223)\n",
            "gb {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 2} (0.8883333333333333, 0.9497888888888889)\n",
            "gb {'n_estimators': 200, 'learning_rate': 0.05, 'max_depth': 2} (0.8833333333333333, 0.9481888888888889)\n"
          ]
        }
      ],
      "source": [
        "settings = [\n",
        "    {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 2},\n",
        "    {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 2},\n",
        "    {'n_estimators': 200, 'learning_rate': 0.05, 'max_depth': 2},\n",
        "]\n",
        "\n",
        "for s in settings:\n",
        "    gb = GradientBoostingClassifier(random_state=0, **s)\n",
        "    gb.fit(Xtr, ytr)\n",
        "    print('gb', s, eval_model(gb, Xva, yva))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3 — XGBoost-style knobs (conceptual)\n",
        "\n",
        "### Task 3.1: Explain what each knob does\n",
        "Write 2-3 bullets each:\n",
        "- subsample\n",
        "- colsample\n",
        "- learning rate\n",
        "- max_depth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **subsample:** \n",
        "    - using a portion of data to train for each tree adds a randomness\n",
        "    - it helps in reducing overfitting.\n",
        "    - it helps with more regularization. \n",
        "\n",
        "- **colsample:**\n",
        "    - using only a random set of feastures to train each tree\n",
        "    - prevets a single dominent feature from dominating\n",
        "    - helps with generalization\n",
        "\n",
        "- **learning_rate:**\n",
        "    - Controls rate at which model is chaged\n",
        "    - it dictates hoe aggresivly model is trained\n",
        "    \n",
        "- **max_depth:**\n",
        "    - Controls how deep each tree can grow.\n",
        "    - deeper the tree more complex model becomes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Submission Checklist\n",
        "- All TODOs completed\n",
        "- Baseline vs RF vs GB compared\n",
        "- OOB score discussed (if available)\n",
        "- Feature importance gotcha explained"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
