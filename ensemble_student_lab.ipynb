{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {"provenance": []},
    "kernelspec": {"name": "python3", "display_name": "Python 3"},
    "language_info": {"name": "python"}
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ensemble Methods & Boosting — Student Lab\n",
        "\n",
        "Week 4 introduces sklearn models, but you must still explain *why* they work (bias/variance)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification, load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "def check(name: str, cond: bool):\n",
        "    if not cond:\n",
        "        raise AssertionError(f'Failed: {name}')\n",
        "    print(f'OK: {name}')\n",
        "\n",
        "rng = np.random.default_rng(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 0 — Dataset (synthetic default, real optional)\n",
        "\n",
        "### Task 0.1: Choose dataset\n",
        "Use synthetic by default. Optionally switch to breast cancer dataset.\n",
        "\n",
        "# TODO: set `use_real = False` or True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "use_real = False  # TODO\n",
        "\n",
        "if use_real:\n",
        "    data = load_breast_cancer()\n",
        "    X = data.data\n",
        "    y = data.target\n",
        "else:\n",
        "    X, y = make_classification(\n",
        "        n_samples=2000,\n",
        "        n_features=20,\n",
        "        n_informative=8,\n",
        "        n_redundant=4,\n",
        "        class_sep=1.0,\n",
        "        flip_y=0.03,\n",
        "        random_state=0,\n",
        "    )\n",
        "\n",
        "Xtr, Xva, ytr, yva = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
        "check('shapes', Xtr.shape[0]==ytr.shape[0] and Xva.shape[0]==yva.shape[0])\n",
        "Xtr.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1 — Baseline vs Trees vs Random Forest\n",
        "\n",
        "### Task 1.1: Train baseline decision tree vs random forest\n",
        "\n",
        "# TODO: Train:\n",
        "- DecisionTreeClassifier(max_depth=?)\n",
        "- RandomForestClassifier(n_estimators=?, max_depth=?, oob_score=True, bootstrap=True)\n",
        "\n",
        "Compute accuracy + ROC-AUC on validation.\n",
        "\n",
        "**Checkpoint:** Why does bagging reduce variance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO\n",
        "tree = ...\n",
        "rf = ...\n",
        "\n",
        "tree.fit(Xtr, ytr)\n",
        "rf.fit(Xtr, ytr)\n",
        "\n",
        "def eval_model(clf, X, y):\n",
        "    pred = clf.predict(X)\n",
        "    acc = accuracy_score(y, pred)\n",
        "    # many sklearn classifiers have predict_proba; handle if not\n",
        "    if hasattr(clf, 'predict_proba'):\n",
        "        proba = clf.predict_proba(X)[:, 1]\n",
        "        auc = roc_auc_score(y, proba)\n",
        "    else:\n",
        "        auc = float('nan')\n",
        "    return acc, auc\n",
        "\n",
        "print('tree', eval_model(tree, Xva, yva))\n",
        "print('rf  ', eval_model(rf, Xva, yva))\n",
        "\n",
        "if hasattr(rf, 'oob_score_'):\n",
        "    print('rf oob_score', rf.oob_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1.2: Feature importance gotcha\n",
        "\n",
        "Inspect `feature_importances_` and explain why correlated features can distort importances.\n",
        "\n",
        "# TODO: print top 10 features by importance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO\n",
        "imp = rf.feature_importances_\n",
        "top = np.argsort(-imp)[:10]\n",
        "print('top idx', top)\n",
        "print('top importances', imp[top])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2 — Gradient Boosting\n",
        "\n",
        "### Task 2.1: Train GradientBoostingClassifier\n",
        "\n",
        "# TODO: Train GB with different n_estimators and learning_rate and compare.\n",
        "\n",
        "**Checkpoint:** Why can boosting overfit with too many estimators?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "settings = [\n",
        "    {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 2},\n",
        "    {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 2},\n",
        "    {'n_estimators': 200, 'learning_rate': 0.05, 'max_depth': 2},\n",
        "]\n",
        "\n",
        "for s in settings:\n",
        "    gb = GradientBoostingClassifier(random_state=0, **s)\n",
        "    gb.fit(Xtr, ytr)\n",
        "    print('gb', s, eval_model(gb, Xva, yva))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3 — XGBoost-style knobs (conceptual)\n",
        "\n",
        "### Task 3.1: Explain what each knob does\n",
        "Write 2-3 bullets each:\n",
        "- subsample\n",
        "- colsample\n",
        "- learning rate\n",
        "- max_depth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **subsample:**\n",
        "- **colsample:**\n",
        "- **learning_rate:**\n",
        "- **max_depth:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Submission Checklist\n",
        "- All TODOs completed\n",
        "- Baseline vs RF vs GB compared\n",
        "- OOB score discussed (if available)\n",
        "- Feature importance gotcha explained"
      ]
    }
  ]
}
